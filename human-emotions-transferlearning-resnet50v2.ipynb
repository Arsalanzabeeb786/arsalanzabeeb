{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":1351797,"datasetId":786787,"databundleVersionId":1384195},{"sourceType":"modelInstanceVersion","sourceId":80921,"databundleVersionId":9180880,"modelInstanceId":68000}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#  Transfer Learning - ResNet50","metadata":{}},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport imghdr\n\nimport seaborn as sns\n\nfrom PIL import Image\n\n\nimport tensorflow as tf\nfrom tensorflow.keras.applications import ResNet50V2\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import BatchNormalization, GlobalAveragePooling2D, Dense, Dropout, Input,Activation\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard, CSVLogger\nfrom tensorflow.keras.optimizers import Adam, Adamax\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\nfrom sklearn.utils.class_weight import compute_class_weight","metadata":{"execution":{"iopub.status.busy":"2024-07-23T10:56:13.322555Z","iopub.execute_input":"2024-07-23T10:56:13.322856Z","iopub.status.idle":"2024-07-23T10:56:27.150805Z","shell.execute_reply.started":"2024-07-23T10:56:13.322830Z","shell.execute_reply":"2024-07-23T10:56:27.149999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # To minimize TensorFlow logging output\nos.environ['TF_ENABLE_ONEDNN_OPTS'] = '1'  # Enable oneDNN optimizations\n\n\ntf.config.optimizer.set_jit(True)  # Enable XLA","metadata":{"execution":{"iopub.status.busy":"2024-07-23T10:56:30.026493Z","iopub.execute_input":"2024-07-23T10:56:30.027426Z","iopub.status.idle":"2024-07-23T10:56:30.032540Z","shell.execute_reply.started":"2024-07-23T10:56:30.027391Z","shell.execute_reply":"2024-07-23T10:56:30.031494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dir = '/kaggle/input/fer2013/train'\ntest_dir = '/kaggle/input/fer2013/test'\n\n# Data augmentation configuration for training data\ntrain_datagen = ImageDataGenerator(rescale=1/255,                # Rescale pixel values to [0, 1]\n                                  rotation_range=40,            # Randomly rotate images in the range (degrees, 0 to 180)\n                                  width_shift_range=0.2,        # Randomly horizontally shift images\n                                  height_shift_range=0.2,       # Randomly vertically shift images\n                                  shear_range=0.2,              # Apply shearing transformations\n                                  zoom_range=0.1,               # Randomly zoom image\n                                  horizontal_flip=True,         # Randomly flip images horizontally\n                                  fill_mode='nearest'           # Strategy used for filling in newly created pixels\n)\n\n\n# Rescaling for validation/test data (without further data augmentation)\ntest_datagen = ImageDataGenerator(\n                                  rescale=1/255                 # Rescale pixel values to [0, 1]\n                              )\n\n# Creating data generators for training\ntrain_generator = train_datagen.flow_from_directory(\n                                                    train_dir,\n                                                    target_size=(224, 224),       # Resize images to 224x224 for model input\n                                                    color_mode='rgb',             # Images will be converted to RGB\n                                                    class_mode='categorical',     # For multi-class classification\n                                                    batch_size=32                 # Size of the batches of data\n                                                )\n\n# Creating data generators for testing/validation\ntest_generator = test_datagen.flow_from_directory(\n                                                  test_dir,\n                                                  target_size=(224, 224),       # Resize images to 224x224 for model input\n                                                  color_mode='rgb',             # Images will be converted to RGB\n                                                  class_mode='categorical',     # For multi-class classification\n                                                  batch_size=32                 # Size of the batches of data\n                                              )","metadata":{"execution":{"iopub.status.busy":"2024-07-23T10:56:36.426461Z","iopub.execute_input":"2024-07-23T10:56:36.427266Z","iopub.status.idle":"2024-07-23T10:56:56.687421Z","shell.execute_reply.started":"2024-07-23T10:56:36.427236Z","shell.execute_reply":"2024-07-23T10:56:56.686690Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract class labels for all instances in the training dataset\nclasses = np.array(train_generator.classes)\n\n# Calculate class weights to handle imbalances in the training data\n# 'balanced' mode automatically adjusts weights inversely proportional to class frequencies\nclass_weights = compute_class_weight(\n    class_weight='balanced',  # Strategy to balance classes\n    classes=np.unique(classes),  # Unique class labels\n    y=classes  # Class labels for each instance in the training dataset\n)\n\n# Create a dictionary mapping class indices to their calculated weights\nclass_weights_dict = dict(enumerate(class_weights))\n\n# Output the class weights dictionary\nprint(\"Class Weights Dictionary:\", class_weights_dict)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T11:07:39.743473Z","iopub.execute_input":"2024-07-23T11:07:39.744438Z","iopub.status.idle":"2024-07-23T11:07:39.765659Z","shell.execute_reply.started":"2024-07-23T11:07:39.744393Z","shell.execute_reply":"2024-07-23T11:07:39.764660Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.applications import ResNet50V2\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import BatchNormalization, GlobalAveragePooling2D, Dense, Dropout\nfrom tensorflow.keras.optimizers import Adamax\n\ninput_shape = (224, 224, 3)\n\nbase_model = ResNet50V2(include_top=False, weights='imagenet', input_shape=input_shape)\n\nmodel = Sequential([\n                    base_model,\n                    BatchNormalization(),\n                    GlobalAveragePooling2D(),\n                    Dense(512, activation='relu'),\n                    Dropout(0.1),\n                    Dense(256, activation='relu'),\n                    Dropout(0.1),\n                    Dense(128, activation='relu'),\n                    Dropout(0.1),\n                    Dense(7, activation='softmax')\n                  ])\n\noptimizer = Adamax(learning_rate=0.0001)\n\nmodel.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n","metadata":{"execution":{"iopub.status.busy":"2024-07-23T11:15:37.710086Z","iopub.execute_input":"2024-07-23T11:15:37.710423Z","iopub.status.idle":"2024-07-23T11:15:38.799117Z","shell.execute_reply.started":"2024-07-23T11:15:37.710398Z","shell.execute_reply":"2024-07-23T11:15:38.798099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_model.summary()","metadata":{"execution":{"iopub.status.busy":"2024-07-23T11:17:51.812360Z","iopub.execute_input":"2024-07-23T11:17:51.812761Z","iopub.status.idle":"2024-07-23T11:17:52.072047Z","shell.execute_reply.started":"2024-07-23T11:17:51.812729Z","shell.execute_reply":"2024-07-23T11:17:52.071179Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2024-07-23T11:16:55.423104Z","iopub.execute_input":"2024-07-23T11:16:55.423825Z","iopub.status.idle":"2024-07-23T11:16:55.455411Z","shell.execute_reply.started":"2024-07-23T11:16:55.423787Z","shell.execute_reply":"2024-07-23T11:16:55.454550Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate the model on the test data to get the initial loss and accuracy\ninitial_loss, initial_accuracy = model.evaluate(test_generator, verbose=2)\n\n# Print the initial loss and accuracy\nprint(f\"Initial loss: {initial_loss}\")\nprint(f\"Initial accuracy: {initial_accuracy}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-23T11:15:55.743857Z","iopub.execute_input":"2024-07-23T11:15:55.744215Z","iopub.status.idle":"2024-07-23T11:16:16.568692Z","shell.execute_reply.started":"2024-07-23T11:15:55.744187Z","shell.execute_reply":"2024-07-23T11:16:16.567708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from tensorflow.keras.models import load_model","metadata":{"execution":{"iopub.status.busy":"2024-07-23T02:14:58.955444Z","iopub.execute_input":"2024-07-23T02:14:58.955813Z","iopub.status.idle":"2024-07-23T02:14:58.960464Z","shell.execute_reply.started":"2024-07-23T02:14:58.955783Z","shell.execute_reply":"2024-07-23T02:14:58.959429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''model_path = '/kaggle/input/resnet50_100epochs_human_emotions/tensorflow2/base100epochs/1/ResNet50_Transfer_Learning.keras'\nResNet50_model_v2 = load_model(model_path)\noptimizer = Adam(learning_rate=0.0001)\nResNet50_model_v2.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Print the model summary\nResNet50_model_v2.summary() '''","metadata":{"execution":{"iopub.status.busy":"2024-07-23T02:15:02.325044Z","iopub.execute_input":"2024-07-23T02:15:02.325404Z","iopub.status.idle":"2024-07-23T02:15:11.641107Z","shell.execute_reply.started":"2024-07-23T02:15:02.325375Z","shell.execute_reply":"2024-07-23T02:15:11.640274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# File path for the model checkpoint\ncnn_path = '/kaggle/working/'\nname = 'ResNet50_model_v1_Transfer_Learning.keras'\nchk_path = os.path.join(cnn_path, name)\n\n# Callback to save the model checkpoint\ncheckpoint = ModelCheckpoint(filepath=chk_path,\n                             save_best_only=True,\n                             verbose=1,\n                             mode='min',\n                             monitor='val_loss')\n'''\n# Callback for early stopping\nearlystop = EarlyStopping(monitor='val_loss',\n                          min_delta=0,\n                          patience=8,\n                          verbose=1,\n                          restore_best_weights=True)\n                          '''\n\n# Callback to reduce learning rate\nreduce_lr = ReduceLROnPlateau(monitor='val_loss',\n                              factor=0.2,\n                              patience=8,\n                              verbose=1,\n                              min_delta=0.0001)\n\n# Callback to log training data to a CSV file\ncsv_logger = CSVLogger(os.path.join(cnn_path,'ResNet50_model_v1_training.log'))\n\n# Aggregating all callbacks into a list\ncallbacks = [checkpoint,csv_logger,reduce_lr]  # Adjusted as per your use-case","metadata":{"execution":{"iopub.status.busy":"2024-07-23T11:19:02.546738Z","iopub.execute_input":"2024-07-23T11:19:02.547399Z","iopub.status.idle":"2024-07-23T11:19:02.555665Z","shell.execute_reply.started":"2024-07-23T11:19:02.547365Z","shell.execute_reply":"2024-07-23T11:19:02.554690Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_history = model.fit(\n                          train_generator,\n                          epochs=200,\n                          validation_data=test_generator, \n                          class_weight=class_weights_dict,\n                          callbacks = callbacks\n                        )","metadata":{"execution":{"iopub.status.busy":"2024-07-23T11:25:30.617881Z","iopub.execute_input":"2024-07-23T11:25:30.618388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nimport matplotlib.pyplot as plt\n\n# Given log data\nlog_data = \"\"\"\nEpoch 1/200\n/opt/conda/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\nW0000 00:00:1721734001.086687     186 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n 96/898 ━━━━━━━━━━━━━━━━━━━━ 11:16 844ms/step - accuracy: 0.1620 - loss: 2.0809\nW0000 00:00:1721734081.416767     186 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 526ms/step - accuracy: 0.2096 - loss: 1.9035\nEpoch 1: val_loss improved from inf to 1.50390, saving model to /kaggle/working/ResNet50_model_v1_Transfer_Learning.keras\n898/898 ━━━━━━━━━━━━━━━━━━━━ 559s 546ms/step - accuracy: 0.2097 - loss: 1.9034 - val_accuracy: 0.4294 - val_loss: 1.5039 - learning_rate: 1.0000e-04\nEpoch 2/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 357ms/step - accuracy: 0.3889 - loss: 1.5793\nEpoch 2: val_loss improved from 1.50390 to 1.28939, saving model to /kaggle/working/ResNet50_model_v1_Transfer_Learning.keras\n898/898 ━━━━━━━━━━━━━━━━━━━━ 339s 375ms/step - accuracy: 0.3890 - loss: 1.5792 - val_accuracy: 0.5103 - val_loss: 1.2894 - learning_rate: 1.0000e-04\nEpoch 3/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 356ms/step - accuracy: 0.4752 - loss: 1.3601\nEpoch 3: val_loss improved from 1.28939 to 1.17683, saving model to /kaggle/working/ResNet50_model_v1_Transfer_Learning.keras\n898/898 ━━━━━━━━━━━━━━━━━━━━ 338s 374ms/step - accuracy: 0.4752 - loss: 1.3601 - val_accuracy: 0.5493 - val_loss: 1.1768 - learning_rate: 1.0000e-04\nEpoch 4/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 356ms/step - accuracy: 0.5172 - loss: 1.2243\nEpoch 4: val_loss improved from 1.17683 to 1.11981, saving model to /kaggle/working/ResNet50_model_v1_Transfer_Learning.keras\n898/898 ━━━━━━━━━━━━━━━━━━━━ 338s 374ms/step - accuracy: 0.5172 - loss: 1.2244 - val_accuracy: 0.5770 - val_loss: 1.1198 - learning_rate: 1.0000e-04\nEpoch 5/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 355ms/step - accuracy: 0.5408 - loss: 1.1691\nEpoch 5: val_loss improved from 1.11981 to 1.10193, saving model to /kaggle/working/ResNet50_model_v1_Transfer_Learning.keras\n898/898 ━━━━━━━━━━━━━━━━━━━━ 338s 373ms/step - accuracy: 0.5408 - loss: 1.1691 - val_accuracy: 0.5857 - val_loss: 1.1019 - learning_rate: 1.0000e-04\nEpoch 6/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 355ms/step - accuracy: 0.5739 - loss: 1.1088\nEpoch 6: val_loss improved from 1.10193 to 1.05540, saving model to /kaggle/working/ResNet50_model_v1_Transfer_Learning.keras\n898/898 ━━━━━━━━━━━━━━━━━━━━ 338s 373ms/step - accuracy: 0.5739 - loss: 1.1088 - val_accuracy: 0.6016 - val_loss: 1.0554 - learning_rate: 1.0000e-04\nEpoch 7/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 352ms/step - accuracy: 0.5839 - loss: 1.0497\nEpoch 7: val_loss improved from 1.05540 to 1.02078, saving model to /kaggle/working/ResNet50_model_v1_Transfer_Learning.keras\n898/898 ━━━━━━━━━━━━━━━━━━━━ 335s 370ms/step - accuracy: 0.5839 - loss: 1.0497 - val_accuracy: 0.6169 - val_loss: 1.0208 - learning_rate: 1.0000e-04\nEpoch 8/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 358ms/step - accuracy: 0.6006 - loss: 1.0042\nEpoch 8: val_loss improved from 1.02078 to 1.00239, saving model to /kaggle/working/ResNet50_model_v1_Transfer_Learning.keras\n898/898 ━━━━━━━━━━━━━━━━━━━━ 341s 376ms/step - accuracy: 0.6006 - loss: 1.0043 - val_accuracy: 0.6290 - val_loss: 1.0024 - learning_rate: 1.0000e-04\nEpoch 9/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 358ms/step - accuracy: 0.6079 - loss: 0.9859\nEpoch 9: val_loss improved from 1.00239 to 0.98856, saving model to /kaggle/working/ResNet50_model_v1_Transfer_Learning.keras\n898/898 ━━━━━━━━━━━━━━━━━━━━ 341s 376ms/step - accuracy: 0.6079 - loss: 0.9859 - val_accuracy: 0.6303 - val_loss: 0.9886 - learning_rate: 1.0000e-04\nEpoch 10/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 361ms/step - accuracy: 0.6204 - loss: 0.9644\nEpoch 10: val_loss did not improve from 0.98856\n898/898 ━━━━━━━━━━━━━━━━━━━━ 341s 377ms/step - accuracy: 0.6204 - loss: 0.9644 - val_accuracy: 0.6337 - val_loss: 1.0109 - learning_rate: 1.0000e-04\nEpoch 11/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 361ms/step - accuracy: 0.6355 - loss: 0.9242\nEpoch 11: val_loss improved from 0.98856 to 0.98541, saving model to /kaggle/working/ResNet50_model_v1_Transfer_Learning.keras\n898/898 ━━━━━━━━━━━━━━━━━━━━ 343s 379ms/step - accuracy: 0.6355 - loss: 0.9242 - val_accuracy: 0.6404 - val_loss: 0.9854 - learning_rate: 1.0000e-04\nEpoch 12/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 362ms/step - accuracy: 0.6403 - loss: 0.9036\nEpoch 12: val_loss improved from 0.98541 to 0.96364, saving model to /kaggle/working/ResNet50_model_v1_Transfer_Learning.keras\n898/898 ━━━━━━━━━━━━━━━━━━━━ 344s 380ms/step - accuracy: 0.6403 - loss: 0.9036 - val_accuracy: 0.6460 - val_loss: 0.9636 - learning_rate: 1.0000e-04\nEpoch 13/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 358ms/step - accuracy: 0.6444 - loss: 0.8777\nEpoch 13: val_loss improved from 0.96364 to 0.94278, saving model to /kaggle/working/ResNet50_model_v1_Transfer_Learning.keras\n898/898 ━━━━━━━━━━━━━━━━━━━━ 378s 376ms/step - accuracy: 0.6444 - loss: 0.8777 - val_accuracy: 0.6519 - val_loss: 0.9428 - learning_rate: 1.0000e-04\nEpoch 14/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 360ms/step - accuracy: 0.6634 - loss: 0.8346\nEpoch 14: val_loss did not improve from 0.94278\n898/898 ━━━━━━━━━━━━━━━━━━━━ 340s 376ms/step - accuracy: 0.6634 - loss: 0.8346 - val_accuracy: 0.6523 - val_loss: 0.9548 - learning_rate: 1.0000e-04\nEpoch 15/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 355ms/step - accuracy: 0.6661 - loss: 0.8240\nEpoch 15: val_loss did not improve from 0.94278\n898/898 ━━━━━━━━━━━━━━━━━━━━ 336s 371ms/step - accuracy: 0.6661 - loss: 0.8240 - val_accuracy: 0.6555 - val_loss: 0.9549 - learning_rate: 1.0000e-04\nEpoch 16/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 358ms/step - accuracy: 0.6641 - loss: 0.8351\nEpoch 16: val_loss did not improve from 0.94278\n898/898 ━━━━━━━━━━━━━━━━━━━━ 339s 374ms/step - accuracy: 0.6641 - loss: 0.8351 - val_accuracy: 0.6566 - val_loss: 0.9522 - learning_rate: 1.0000e-04\nEpoch 17/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 360ms/step - accuracy: 0.6832 - loss: 0.7843\nEpoch 17: val_loss did not improve from 0.94278\n898/898 ━━━━━━━━━━━━━━━━━━━━ 341s 377ms/step - accuracy: 0.6832 - loss: 0.7843 - val_accuracy: 0.6662 - val_loss: 0.9452 - learning_rate: 1.0000e-04\nEpoch 18/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 364ms/step - accuracy: 0.6950 - loss: 0.7577\nEpoch 18: val_loss did not improve from 0.94278\n898/898 ━━━━━━━━━━━━━━━━━━━━ 344s 380ms/step - accuracy: 0.6950 - loss: 0.7577 - val_accuracy: 0.6669 - val_loss: 0.9577 - learning_rate: 1.0000e-04\nEpoch 19/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 360ms/step - accuracy: 0.6920 - loss: 0.7524\nEpoch 19: val_loss did not improve from 0.94278\n898/898 ━━━━━━━━━━━━━━━━━━━━ 340s 375ms/step - accuracy: 0.6920 - loss: 0.7524 - val_accuracy: 0.6638 - val_loss: 0.9533 - learning_rate: 1.0000e-04\nEpoch 20/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 361ms/step - accuracy: 0.7050 - loss: 0.7272\nEpoch 20: val_loss improved from 0.94278 to 0.92223, saving model to /kaggle/working/ResNet50_model_v1_Transfer_Learning.keras\n898/898 ━━━━━━━━━━━━━━━━━━━━ 343s 379ms/step - accuracy: 0.7049 - loss: 0.7272 - val_accuracy: 0.6718 - val_loss: 0.9222 - learning_rate: 1.0000e-04\nEpoch 21/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 359ms/step - accuracy: 0.7066 - loss: 0.7235\nEpoch 21: val_loss did not improve from 0.92223\n898/898 ━━━━━━━━━━━━━━━━━━━━ 339s 375ms/step - accuracy: 0.7066 - loss: 0.7235 - val_accuracy: 0.6726 - val_loss: 0.9475 - learning_rate: 1.0000e-04\nEpoch 22/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 365ms/step - accuracy: 0.7142 - loss: 0.7101\nEpoch 22: val_loss did not improve from 0.92223\n898/898 ━━━━━━━━━━━━━━━━━━━━ 344s 381ms/step - accuracy: 0.7142 - loss: 0.7101 - val_accuracy: 0.6790 - val_loss: 0.9493 - learning_rate: 1.0000e-04\nEpoch 23/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 370ms/step - accuracy: 0.7162 - loss: 0.7036\nEpoch 23: val_loss did not improve from 0.92223\n898/898 ━━━━━━━━━━━━━━━━━━━━ 350s 386ms/step - accuracy: 0.7162 - loss: 0.7036 - val_accuracy: 0.6739 - val_loss: 0.9590 - learning_rate: 1.0000e-04\nEpoch 24/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 363ms/step - accuracy: 0.7311 - loss: 0.6712\nEpoch 24: val_loss did not improve from 0.92223\n898/898 ━━━━━━━━━━━━━━━━━━━━ 343s 379ms/step - accuracy: 0.7311 - loss: 0.6712 - val_accuracy: 0.6780 - val_loss: 1.0044 - learning_rate: 1.0000e-04\nEpoch 25/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 360ms/step - accuracy: 0.7434 - loss: 0.6367\nEpoch 25: val_loss did not improve from 0.92223\n898/898 ━━━━━━━━━━━━━━━━━━━━ 340s 376ms/step - accuracy: 0.7434 - loss: 0.6367 - val_accuracy: 0.6815 - val_loss: 0.9533 - learning_rate: 1.0000e-04\nEpoch 26/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 361ms/step - accuracy: 0.7417 - loss: 0.6396\nEpoch 26: val_loss did not improve from 0.92223\n898/898 ━━━━━━━━━━━━━━━━━━━━ 341s 377ms/step - accuracy: 0.7417 - loss: 0.6396 - val_accuracy: 0.6838 - val_loss: 0.9579 - learning_rate: 1.0000e-04\nEpoch 27/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 364ms/step - accuracy: 0.7482 - loss: 0.6166\nEpoch 27: val_loss did not improve from 0.92223\n898/898 ━━━━━━━━━━━━━━━━━━━━ 344s 380ms/step - accuracy: 0.7482 - loss: 0.6166 - val_accuracy: 0.6842 - val_loss: 0.9632 - learning_rate: 1.0000e-04\nEpoch 28/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 365ms/step - accuracy: 0.7577 - loss: 0.5998\nEpoch 28: val_loss did not improve from 0.92223\n\nEpoch 28: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n898/898 ━━━━━━━━━━━━━━━━━━━━ 344s 380ms/step - accuracy: 0.7577 - loss: 0.5998 - val_accuracy: 0.6849 - val_loss: 0.9789 - learning_rate: 1.0000e-04\nEpoch 29/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 365ms/step - accuracy: 0.7690 - loss: 0.5678\nEpoch 29: val_loss did not improve from 0.92223\n898/898 ━━━━━━━━━━━━━━━━━━━━ 345s 381ms/step - accuracy: 0.7690 - loss: 0.5678 - val_accuracy: 0.6865 - val_loss: 0.9960 - learning_rate: 2.0000e-05\nEpoch 30/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 364ms/step - accuracy: 0.7832 - loss: 0.5382\nEpoch 30: val_loss did not improve from 0.92223\n898/898 ━━━━━━━━━━━━━━━━━━━━ 344s 380ms/step - accuracy: 0.7832 - loss: 0.5382 - val_accuracy: 0.6902 - val_loss: 0.9960 - learning_rate: 2.0000e-05\nEpoch 31/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 362ms/step - accuracy: 0.7825 - loss: 0.5411\nEpoch 31: val_loss did not improve from 0.92223\n898/898 ━━━━━━━━━━━━━━━━━━━━ 342s 378ms/step - accuracy: 0.7825 - loss: 0.5411 - val_accuracy: 0.6911 - val_loss: 0.9964 - learning_rate: 2.0000e-05\nEpoch 32/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 371ms/step - accuracy: 0.7858 - loss: 0.5295\nEpoch 32: val_loss did not improve from 0.92223\n898/898 ━━━━━━━━━━━━━━━━━━━━ 390s 387ms/step - accuracy: 0.7858 - loss: 0.5295 - val_accuracy: 0.6899 - val_loss: 1.0031 - learning_rate: 2.0000e-05\nEpoch 33/200\n796/898 ━━━━━━━━━━━━━━━━━━━━ 37s 373ms/step - accuracy: 0.7939 - loss: 0.5116\nIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 366ms/step - accuracy: 0.8004 - loss: 0.4947\nEpoch 36: val_loss did not improve from 0.92223\n\nEpoch 36: ReduceLROnPlateau reducing learning rate to 3.999999898951501e-06.\n898/898 ━━━━━━━━━━━━━━━━━━━━ 345s 382ms/step - accuracy: 0.8004 - loss: 0.4947 - val_accuracy: 0.6904 - val_loss: 1.0344 - learning_rate: 2.0000e-05\nEpoch 37/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 362ms/step - accuracy: 0.7988 - loss: 0.4900\nEpoch 37: val_loss did not improve from 0.92223\n898/898 ━━━━━━━━━━━━━━━━━━━━ 342s 378ms/step - accuracy: 0.7988 - loss: 0.4900 - val_accuracy: 0.6882 - val_loss: 1.0364 - learning_rate: 4.0000e-06\nEpoch 38/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 363ms/step - accuracy: 0.8049 - loss: 0.4895\nEpoch 38: val_loss did not improve from 0.92223\n898/898 ━━━━━━━━━━━━━━━━━━━━ 343s 379ms/step - accuracy: 0.8049 - loss: 0.4895 - val_accuracy: 0.6914 - val_loss: 1.0363 - learning_rate: 4.0000e-06\nEpoch 39/200\n482/898 ━━━━━━━━━━━━━━━━━━━━ 2:29 360ms/step - accuracy: 0.8023 - loss: 0.4930\nIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 356ms/step - accuracy: 0.8087 - loss: 0.4656\nEpoch 65: val_loss did not improve from 0.92223\n898/898 ━━━━━━━━━━━━━━━━━━━━ 337s 372ms/step - accuracy: 0.8087 - loss: 0.4656 - val_accuracy: 0.6911 - val_loss: 1.0463 - learning_rate: 3.2000e-08\nEpoch 66/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 354ms/step - accuracy: 0.8136 - loss: 0.4553\nEpoch 66: val_loss did not improve from 0.92223\n898/898 ━━━━━━━━━━━━━━━━━━━━ 335s 370ms/step - accuracy: 0.8136 - loss: 0.4553 - val_accuracy: 0.6914 - val_loss: 1.0485 - learning_rate: 3.2000e-08\nEpoch 67/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 355ms/step - accuracy: 0.8149 - loss: 0.4538\nEpoch 67: val_loss did not improve from 0.92223\n898/898 ━━━━━━━━━━━━━━━━━━━━ 335s 370ms/step - accuracy: 0.8149 - loss: 0.4538 - val_accuracy: 0.6917 - val_loss: 1.0458 - learning_rate: 3.2000e-08\nEpoch 68/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 355ms/step - accuracy: 0.8124 - loss: 0.4667\nEpoch 68: val_loss did not improve from 0.92223\n\nEpoch 68: ReduceLROnPlateau reducing learning rate to 6.399999818995639e-09.\n898/898 ━━━━━━━━━━━━━━━━━━━━ 335s 370ms/step - accuracy: 0.8124 - loss: 0.4667 - val_accuracy: 0.6917 - val_loss: 1.0461 - learning_rate: 3.2000e-08\nEpoch 69/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 356ms/step - accuracy: 0.8148 - loss: 0.4639\nEpoch 69: val_loss did not improve from 0.92223\n898/898 ━━━━━━━━━━━━━━━━━━━━ 336s 372ms/step - accuracy: 0.8148 - loss: 0.4639 - val_accuracy: 0.6930 - val_loss: 1.0458 - learning_rate: 6.4000e-09\nEpoch 70/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 356ms/step - accuracy: 0.8061 - loss: 0.4730\nEpoch 70: val_loss did not improve from 0.92223\n898/898 ━━━━━━━━━━━━━━━━━━━━ 336s 372ms/step - accuracy: 0.8061 - loss: 0.4729 - val_accuracy: 0.6909 - val_loss: 1.0467 - learning_rate: 6.4000e-09\nEpoch 71/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 355ms/step - accuracy: 0.8100 - loss: 0.4624\nEpoch 71: val_loss did not improve from 0.92223\n898/898 ━━━━━━━━━━━━━━━━━━━━ 336s 371ms/step - accuracy: 0.8100 - loss: 0.4624 - val_accuracy: 0.6921 - val_loss: 1.0472 - learning_rate: 6.4000e-09\nEpoch 72/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 354ms/step - accuracy: 0.8133 - loss: 0.4640\nEpoch 72: val_loss did not improve from 0.92223\n898/898 ━━━━━━━━━━━━━━━━━━━━ 335s 370ms/step - accuracy: 0.8132 - loss: 0.4640 - val_accuracy: 0.6914 - val_loss: 1.0485 - learning_rate: 6.4000e-09\nEpoch 73/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 370ms/step - accuracy: 0.8098 - loss: 0.4650\nEpoch 73: val_loss did not improve from 0.92223\n898/898 ━━━━━━━━━━━━━━━━━━━━ 349s 386ms/step - accuracy: 0.8098 - loss: 0.4650 - val_accuracy: 0.6930 - val_loss: 1.0456 - learning_rate: 6.4000e-09\nEpoch 74/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 354ms/step - accuracy: 0.8085 - loss: 0.4651\nEpoch 74: val_loss did not improve from 0.92223\n898/898 ━━━━━━━━━━━━━━━━━━━━ 335s 370ms/step - accuracy: 0.8085 - loss: 0.4651 - val_accuracy: 0.6927 - val_loss: 1.0457 - learning_rate: 6.4000e-09\nEpoch 75/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 357ms/step - accuracy: 0.8129 - loss: 0.4617\nEpoch 75: val_loss did not improve from 0.92223\n898/898 ━━━━━━━━━━━━━━━━━━━━ 384s 373ms/step - accuracy: 0.8129 - loss: 0.4617 - val_accuracy: 0.6907 - val_loss: 1.0444 - learning_rate: 6.4000e-09\nEpoch 76/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 358ms/step - accuracy: 0.8077 - loss: 0.4717\nEpoch 76: val_loss did not improve from 0.92223\n\nEpoch 76: ReduceLROnPlateau reducing learning rate to 1.279999928271991e-09.\n898/898 ━━━━━━━━━━━━━━━━━━━━ 338s 374ms/step - accuracy: 0.8077 - loss: 0.4717 - val_accuracy: 0.6918 - val_loss: 1.0448 - learning_rate: 6.4000e-09\nEpoch 77/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 354ms/step - accuracy: 0.8108 - loss: 0.4598\nEpoch 77: val_loss did not improve from 0.92223\n898/898 ━━━━━━━━━━━━━━━━━━━━ 335s 370ms/step - accuracy: 0.8108 - loss: 0.4598 - val_accuracy: 0.6927 - val_loss: 1.0463 - learning_rate: 1.2800e-09\nEpoch 78/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 358ms/step - accuracy: 0.8134 - loss: 0.4599\nEpoch 78: val_loss did not improve from 0.92223\n898/898 ━━━━━━━━━━━━━━━━━━━━ 339s 374ms/step - accuracy: 0.8134 - loss: 0.4599 - val_accuracy: 0.6923 - val_loss: 1.0440 - learning_rate: 1.2800e-09\nEpoch 79/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 357ms/step - accuracy: 0.8100 - loss: 0.4683\nEpoch 79: val_loss did not improve from 0.92223\n898/898 ━━━━━━━━━━━━━━━━━━━━ 337s 373ms/step - accuracy: 0.8100 - loss: 0.4683 - val_accuracy: 0.6916 - val_loss: 1.0494 - learning_rate: 1.2800e-09\nEpoch 80/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 356ms/step - accuracy: 0.8169 - loss: 0.4503\nEpoch 80: val_loss did not improve from 0.92223\n898/898 ━━━━━━━━━━━━━━━━━━━━ 337s 372ms/step - accuracy: 0.8169 - loss: 0.4503 - val_accuracy: 0.6927 - val_loss: 1.0464 - learning_rate: 1.2800e-09\nEpoch 81/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 376ms/step - accuracy: 0.8078 - loss: 0.4780\nEpoch 81: val_loss did not improve from 0.92223\n898/898 ━━━━━━━━━━━━━━━━━━━━ 354s 392ms/step - accuracy: 0.8078 - loss: 0.4779 - val_accuracy: 0.6923 - val_loss: 1.0438 - learning_rate: 1.2800e-09\nEpoch 82/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 379ms/step - accuracy: 0.8062 - loss: 0.4767\nEpoch 82: val_loss did not improve from 0.92223\n898/898 ━━━━━━━━━━━━━━━━━━━━ 358s 395ms/step - accuracy: 0.8062 - loss: 0.4767 - val_accuracy: 0.6923 - val_loss: 1.0451 - learning_rate: 1.2800e-09\nEpoch 83/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 371ms/step - accuracy: 0.8126 - loss: 0.4650\nEpoch 83: val_loss did not improve from 0.92223\n898/898 ━━━━━━━━━━━━━━━━━━━━ 350s 387ms/step - accuracy: 0.8126 - loss: 0.4650 - val_accuracy: 0.6906 - val_loss: 1.0463 - learning_rate: 1.2800e-09\nEpoch 84/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 357ms/step - accuracy: 0.8111 - loss: 0.4615\nEpoch 84: val_loss did not improve from 0.92223\n\nEpoch 84: ReduceLROnPlateau reducing learning rate to 2.55999976772614e-10.\n898/898 ━━━━━━━━━━━━━━━━━━━━ 337s 372ms/step - accuracy: 0.8111 - loss: 0.4615 - val_accuracy: 0.6917 - val_loss: 1.0485 - learning_rate: 1.2800e-09\nEpoch 85/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 354ms/step - accuracy: 0.8111 - loss: 0.4619\nEpoch 85: val_loss did not improve from 0.92223\n898/898 ━━━━━━━━━━━━━━━━━━━━ 335s 370ms/step - accuracy: 0.8111 - loss: 0.4619 - val_accuracy: 0.6920 - val_loss: 1.0484 - learning_rate: 2.5600e-10\nEpoch 86/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 354ms/step - accuracy: 0.8112 - loss: 0.4613\nEpoch 86: val_loss did not improve from 0.92223\n898/898 ━━━━━━━━━━━━━━━━━━━━ 335s 370ms/step - accuracy: 0.8112 - loss: 0.4613 - val_accuracy: 0.6917 - val_loss: 1.0466 - learning_rate: 2.5600e-10\nEpoch 87/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 353ms/step - accuracy: 0.8090 - loss: 0.4747\nEpoch 87: val_loss did not improve from 0.92223\n898/898 ━━━━━━━━━━━━━━━━━━━━ 334s 369ms/step - accuracy: 0.8090 - loss: 0.4747 - val_accuracy: 0.6911 - val_loss: 1.0466 - learning_rate: 2.5600e-10\nEpoch 88/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 357ms/step - accuracy: 0.8150 - loss: 0.4582\nEpoch 88: val_loss did not improve from 0.92223\n898/898 ━━━━━━━━━━━━━━━━━━━━ 337s 373ms/step - accuracy: 0.8150 - loss: 0.4582 - val_accuracy: 0.6913 - val_loss: 1.0456 - learning_rate: 2.5600e-10\nEpoch 89/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 354ms/step - accuracy: 0.8135 - loss: 0.4639\nEpoch 89: val_loss did not improve from 0.92223\n898/898 ━━━━━━━━━━━━━━━━━━━━ 335s 370ms/step - accuracy: 0.8135 - loss: 0.4639 - val_accuracy: 0.6909 - val_loss: 1.0493 - learning_rate: 2.5600e-10\nEpoch 90/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 355ms/step - accuracy: 0.8079 - loss: 0.4659\nEpoch 90: val_loss did not improve from 0.92223\n898/898 ━━━━━━━━━━━━━━━━━━━━ 336s 371ms/step - accuracy: 0.8079 - loss: 0.4659 - val_accuracy: 0.6916 - val_loss: 1.0470 - learning_rate: 2.5600e-10\nEpoch 91/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 385ms/step - accuracy: 0.8091 - loss: 0.4705\nEpoch 91: val_loss did not improve from 0.92223\n898/898 ━━━━━━━━━━━━━━━━━━━━ 362s 400ms/step - accuracy: 0.8091 - loss: 0.4705 - val_accuracy: 0.6910 - val_loss: 1.0480 - learning_rate: 2.5600e-10\nEpoch 92/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 356ms/step - accuracy: 0.8089 - loss: 0.4670\nEpoch 92: val_loss did not improve from 0.92223\n\nEpoch 92: ReduceLROnPlateau reducing learning rate to 5.119999424429978e-11.\n898/898 ━━━━━━━━━━━━━━━━━━━━ 337s 372ms/step - accuracy: 0.8089 - loss: 0.4670 - val_accuracy: 0.6923 - val_loss: 1.0449 - learning_rate: 2.5600e-10\nEpoch 93/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 358ms/step - accuracy: 0.8107 - loss: 0.4718\nEpoch 93: val_loss did not improve from 0.92223\n898/898 ━━━━━━━━━━━━━━━━━━━━ 339s 374ms/step - accuracy: 0.8107 - loss: 0.4718 - val_accuracy: 0.6910 - val_loss: 1.0452 - learning_rate: 5.1200e-11\nEpoch 94/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 360ms/step - accuracy: 0.8140 - loss: 0.4591\nEpoch 94: val_loss did not improve from 0.92223\n898/898 ━━━━━━━━━━━━━━━━━━━━ 340s 376ms/step - accuracy: 0.8140 - loss: 0.4591 - val_accuracy: 0.6914 - val_loss: 1.0445 - learning_rate: 5.1200e-11\nEpoch 95/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 362ms/step - accuracy: 0.8094 - loss: 0.4641\nEpoch 95: val_loss did not improve from 0.92223\n898/898 ━━━━━━━━━━━━━━━━━━━━ 342s 378ms/step - accuracy: 0.8094 - loss: 0.4641 - val_accuracy: 0.6906 - val_loss: 1.0464 - learning_rate: 5.1200e-11\nEpoch 96/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 395ms/step - accuracy: 0.8106 - loss: 0.4591\nEpoch 96: val_loss did not improve from 0.92223\n898/898 ━━━━━━━━━━━━━━━━━━━━ 372s 411ms/step - accuracy: 0.8106 - loss: 0.4591 - val_accuracy: 0.6906 - val_loss: 1.0470 - learning_rate: 5.1200e-11\nEpoch 97/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 394ms/step - accuracy: 0.8114 - loss: 0.4651\nEpoch 97: val_loss did not improve from 0.92223\n898/898 ━━━━━━━━━━━━━━━━━━━━ 371s 410ms/step - accuracy: 0.8114 - loss: 0.4651 - val_accuracy: 0.6913 - val_loss: 1.0489 - learning_rate: 5.1200e-11\nEpoch 98/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 401ms/step - accuracy: 0.8077 - loss: 0.4740\nEpoch 98: val_loss did not improve from 0.92223\n898/898 ━━━━━━━━━━━━━━━━━━━━ 377s 417ms/step - accuracy: 0.8077 - loss: 0.4740 - val_accuracy: 0.6924 - val_loss: 1.0442 - learning_rate: 5.1200e-11\nEpoch 99/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 406ms/step - accuracy: 0.8084 - loss: 0.4760\nEpoch 99: val_loss did not improve from 0.92223\n898/898 ━━━━━━━━━━━━━━━━━━━━ 382s 422ms/step - accuracy: 0.8084 - loss: 0.4759 - val_accuracy: 0.6911 - val_loss: 1.0462 - learning_rate: 5.1200e-11\nEpoch 100/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 387ms/step - accuracy: 0.8062 - loss: 0.4642\nEpoch 100: val_loss did not improve from 0.92223\n\nEpoch 100: ReduceLROnPlateau reducing learning rate to 1.0239999126415712e-11.\n898/898 ━━━━━━━━━━━━━━━━━━━━ 365s 403ms/step - accuracy: 0.8062 - loss: 0.4642 - val_accuracy: 0.6925 - val_loss: 1.0450 - learning_rate: 5.1200e-11\nEpoch 101/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 376ms/step - accuracy: 0.8067 - loss: 0.4629\nEpoch 101: val_loss did not improve from 0.92223\n898/898 ━━━━━━━━━━━━━━━━━━━━ 355s 392ms/step - accuracy: 0.8067 - loss: 0.4629 - val_accuracy: 0.6918 - val_loss: 1.0440 - learning_rate: 1.0240e-11\nEpoch 102/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 375ms/step - accuracy: 0.8127 - loss: 0.4590\nEpoch 102: val_loss did not improve from 0.92223\n898/898 ━━━━━━━━━━━━━━━━━━━━ 354s 391ms/step - accuracy: 0.8127 - loss: 0.4590 - val_accuracy: 0.6907 - val_loss: 1.0479 - learning_rate: 1.0240e-11\nEpoch 103/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 374ms/step - accuracy: 0.8110 - loss: 0.4657\nEpoch 103: val_loss did not improve from 0.92223\n898/898 ━━━━━━━━━━━━━━━━━━━━ 353s 390ms/step - accuracy: 0.8110 - loss: 0.4657 - val_accuracy: 0.6924 - val_loss: 1.0478 - learning_rate: 1.0240e-11\nEpoch 104/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 375ms/step - accuracy: 0.8127 - loss: 0.4647\nEpoch 104: val_loss did not improve from 0.92223\n898/898 ━━━━━━━━━━━━━━━━━━━━ 354s 391ms/step - accuracy: 0.8127 - loss: 0.4647 - val_accuracy: 0.6920 - val_loss: 1.0496 - learning_rate: 1.0240e-11\nEpoch 105/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 375ms/step - accuracy: 0.8095 - loss: 0.4700\nEpoch 105: val_loss did not improve from 0.92223\n898/898 ━━━━━━━━━━━━━━━━━━━━ 354s 391ms/step - accuracy: 0.8095 - loss: 0.4700 - val_accuracy: 0.6910 - val_loss: 1.0464 - learning_rate: 1.0240e-11\nEpoch 106/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 378ms/step - accuracy: 0.8072 - loss: 0.4752\nEpoch 106: val_loss did not improve from 0.92223\n898/898 ━━━━━━━━━━━━━━━━━━━━ 356s 394ms/step - accuracy: 0.8072 - loss: 0.4752 - val_accuracy: 0.6914 - val_loss: 1.0455 - learning_rate: 1.0240e-11\nEpoch 107/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 379ms/step - accuracy: 0.8137 - loss: 0.4609\nEpoch 107: val_loss did not improve from 0.92223\n898/898 ━━━━━━━━━━━━━━━━━━━━ 358s 395ms/step - accuracy: 0.8137 - loss: 0.4609 - val_accuracy: 0.6916 - val_loss: 1.0459 - learning_rate: 1.0240e-11\nEpoch 108/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 376ms/step - accuracy: 0.8094 - loss: 0.4727\nEpoch 108: val_loss did not improve from 0.92223\n\nEpoch 108: ReduceLROnPlateau reducing learning rate to 2.0479997905886727e-12.\n898/898 ━━━━━━━━━━━━━━━━━━━━ 355s 392ms/step - accuracy: 0.8094 - loss: 0.4727 - val_accuracy: 0.6911 - val_loss: 1.0476 - learning_rate: 1.0240e-11\nEpoch 109/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 389ms/step - accuracy: 0.8111 - loss: 0.4667\nEpoch 109: val_loss did not improve from 0.92223\n898/898 ━━━━━━━━━━━━━━━━━━━━ 367s 405ms/step - accuracy: 0.8111 - loss: 0.4667 - val_accuracy: 0.6910 - val_loss: 1.0459 - learning_rate: 2.0480e-12\nEpoch 110/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 393ms/step - accuracy: 0.8136 - loss: 0.4582\nEpoch 110: val_loss did not improve from 0.92223\n898/898 ━━━━━━━━━━━━━━━━━━━━ 370s 409ms/step - accuracy: 0.8136 - loss: 0.4582 - val_accuracy: 0.6923 - val_loss: 1.0471 - learning_rate: 2.0480e-12\nEpoch 111/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 392ms/step - accuracy: 0.8134 - loss: 0.4600\nEpoch 111: val_loss did not improve from 0.92223\n898/898 ━━━━━━━━━━━━━━━━━━━━ 369s 408ms/step - accuracy: 0.8134 - loss: 0.4600 - val_accuracy: 0.6918 - val_loss: 1.0463 - learning_rate: 2.0480e-12\nEpoch 112/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 390ms/step - accuracy: 0.8104 - loss: 0.4701\nEpoch 112: val_loss did not improve from 0.92223\n898/898 ━━━━━━━━━━━━━━━━━━━━ 367s 406ms/step - accuracy: 0.8104 - loss: 0.4701 - val_accuracy: 0.6928 - val_loss: 1.0453 - learning_rate: 2.0480e-12\nEpoch 113/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 397ms/step - accuracy: 0.8085 - loss: 0.4694\nEpoch 113: val_loss did not improve from 0.92223\n898/898 ━━━━━━━━━━━━━━━━━━━━ 373s 412ms/step - accuracy: 0.8085 - loss: 0.4694 - val_accuracy: 0.6914 - val_loss: 1.0441 - learning_rate: 2.0480e-12\nEpoch 114/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 386ms/step - accuracy: 0.8105 - loss: 0.4666\nEpoch 114: val_loss did not improve from 0.92223\n898/898 ━━━━━━━━━━━━━━━━━━━━ 364s 402ms/step - accuracy: 0.8105 - loss: 0.4666 - val_accuracy: 0.6916 - val_loss: 1.0429 - learning_rate: 2.0480e-12\nEpoch 115/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 375ms/step - accuracy: 0.8085 - loss: 0.4674\nEpoch 115: val_loss did not improve from 0.92223\n898/898 ━━━━━━━━━━━━━━━━━━━━ 353s 390ms/step - accuracy: 0.8085 - loss: 0.4674 - val_accuracy: 0.6916 - val_loss: 1.0481 - learning_rate: 2.0480e-12\nEpoch 116/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 388ms/step - accuracy: 0.8086 - loss: 0.4695\nEpoch 116: val_loss did not improve from 0.92223\n\nEpoch 116: ReduceLROnPlateau reducing learning rate to 4.0959995811773456e-13.\n898/898 ━━━━━━━━━━━━━━━━━━━━ 366s 404ms/step - accuracy: 0.8086 - loss: 0.4695 - val_accuracy: 0.6914 - val_loss: 1.0454 - learning_rate: 2.0480e-12\nEpoch 117/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 397ms/step - accuracy: 0.8118 - loss: 0.4675\nEpoch 117: val_loss did not improve from 0.92223\n898/898 ━━━━━━━━━━━━━━━━━━━━ 373s 412ms/step - accuracy: 0.8118 - loss: 0.4675 - val_accuracy: 0.6916 - val_loss: 1.0485 - learning_rate: 4.0960e-13\nEpoch 118/200\n898/898 ━━━━━━━━━━━━━━━━━━━━ 0s 395ms/step - accuracy: 0.8127 - loss: 0.4668\nEpoch 118: val_loss did not improve from 0.92223\n898/898 ━━━━━━━━━━━━━━━━━━━━ 372s 411ms/step - accuracy: 0.8127 - loss: 0.4668 - val_accuracy: 0.6910 - val_loss: 1.0498 - learning_rate: 4.0960e-13\nEpoch 119/200\n787/898 ━━━━━━━━━━━━━━━━━━━━ 44s 402ms/step - accuracy: 0.8126 - loss: 0.4658\n\"\"\"\n\n\n\n# Regular expression to extract the relevant metrics\npattern = r\"Epoch (\\d+)/\\d+.*?accuracy: (\\d+\\.\\d+).*?loss: (\\d+\\.\\d+).*?val_accuracy: (\\d+\\.\\d+).*?val_loss: (\\d+\\.\\d+)\"\n\n# Find all matches\nmatches = re.findall(pattern, log_data, re.DOTALL)\n\n# Extracted data\nepochs = []\nloss = []\nval_loss = []\naccuracy = []\nval_accuracy = []\n\nfor match in matches:\n    epochs.append(int(match[0]))\n    accuracy.append(float(match[1]))\n    loss.append(float(match[2]))\n    val_accuracy.append(float(match[3]))\n    val_loss.append(float(match[4]))\n\n# Plotting\nplt.figure(figsize=(14, 7))\n\n# Loss plot\nplt.subplot(1, 2, 1)\nplt.plot(epochs, loss, label='Training Loss')\nplt.plot(epochs, val_loss, label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss')\nplt.legend()\n\n# Accuracy plot\nplt.subplot(1, 2, 2)\nplt.plot(epochs, accuracy, label='Training Accuracy')\nplt.plot(epochs, val_accuracy, label='Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-07-24T02:22:34.215408Z","iopub.execute_input":"2024-07-24T02:22:34.215825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n!pip install plotly\nimport plotly.graph_objects as go\n\n# Create figure for loss\nfig_loss = go.Figure()\n\n# Add traces for loss\nfig_loss.add_trace(go.Scatter(x=epochs, y=loss, mode='lines+markers', name='Training Loss'))\nfig_loss.add_trace(go.Scatter(x=epochs, y=val_loss, mode='lines+markers', name='Validation Loss'))\n\n# Update layout for loss\nfig_loss.update_layout(title='Training and Validation Loss',\n                       xaxis_title='Epochs',\n                       yaxis_title='Loss')\n\n# Show loss figure\nfig_loss.show()\n\n# Create figure for accuracy\nfig_accuracy = go.Figure()\n\n# Add traces for accuracy\nfig_accuracy.add_trace(go.Scatter(x=epochs, y=accuracy, mode='lines+markers', name='Training Accuracy'))\nfig_accuracy.add_trace(go.Scatter(x=epochs, y=val_accuracy, mode='lines+markers', name='Validation Accuracy'))\n\n# Update layout for accuracy\nfig_accuracy.update_layout(title='Training and Validation Accuracy',\n                           xaxis_title='Epochs',\n                           yaxis_title='Accuracy')\n\n# Show accuracy figure\nfig_accuracy.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-07-24T02:22:39.998892Z","iopub.execute_input":"2024-07-24T02:22:39.999314Z","iopub.status.idle":"2024-07-24T02:23:16.921239Z","shell.execute_reply.started":"2024-07-24T02:22:39.999283Z","shell.execute_reply":"2024-07-24T02:23:16.919901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_training_history(history):\n    \"\"\"\n    Plots the training and validation accuracy and loss.\n\n    Parameters:\n    - history: A Keras History object. Contains the logs from the training process.\n\n    Returns:\n    - None. Displays the matplotlib plots for training/validation accuracy and loss.\n    \"\"\"\n    acc = history.history['accuracy']\n    val_acc = history.history['val_accuracy']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n\n    epochs_range = range(len(acc))\n\n    plt.figure(figsize=(20, 5))\n\n    # Plot training and validation accuracy\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs_range, acc, label='Training Accuracy')\n    plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n    plt.legend(loc='lower right')\n    plt.title('Training and Validation Accuracy')\n\n    # Plot training and validation loss\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs_range, loss, label='Training Loss')\n    plt.plot(epochs_range, val_loss, label='Validation Loss')\n    plt.legend(loc='upper right')\n    plt.title('Training and Validation Loss')\n\n    plt.show()\n    \nplot_training_history(train_history)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate the model on the test data to get the initial loss and accuracy\ninitial_loss, initial_accuracy = model.evaluate(test_generator, verbose=2)\n\n# Print the initial loss and accuracy\nprint(f\"Initial loss: {initial_loss}\")\nprint(f\"Initial accuracy: {initial_accuracy}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate the model on the training data\ntrain_loss, train_accu = model.evaluate(train_generator)\n# Evaluate the model on the test data\ntest_loss, test_accu = model.evaluate(test_generator)\n\n# Print the final training and validation accuracy\nprint(\"Final train accuracy = {:.2f} , validation accuracy = {:.2f}\".format(train_accu * 100, test_accu * 100))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assuming your true_classes and predicted_classes are already defined\ntrue_classes = test_generator.classes\nsteps = int(np.ceil(test_generator.samples / test_generator.batch_size))\npredicted_classes = np.argmax(model.predict(test_generator, steps=steps), axis=1)\nclass_labels = list(test_generator.class_indices.keys())\n\n# Generate the confusion matrix\ncm = confusion_matrix(true_classes, predicted_classes)\n\n# Plotting with seaborn\nplt.figure(figsize=(10, 8))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\nplt.title('Confusion Matrix')\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Printing the classification report\nResNet50_model_v1_report = classification_report(true_classes,\n                               predicted_classes,\n                               target_names=class_labels,\n                               zero_division=0)\nprint(\"Classification Report:\\n\", ResNet50_model_v1_report)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"true_labels = test_generator.classes\npreds = model.predict(test_generator, steps=len(test_generator))\npred_labels = np.argmax(preds, axis=1)\nclasses=list(test_generator.class_indices.keys())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_encoded = pd.get_dummies(true_labels).astype(int).values\npreds_encoded = pd.get_dummies(pred_labels).astype(int).values\n\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(7):\n    fpr[i], tpr[i], _ = roc_curve(y_encoded[:,i], preds_encoded[:,i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\nplt.figure(figsize=(10,5))\ncolors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2']\nfor i, color in enumerate(colors):\n    plt.plot(fpr[i], tpr[i], color=color, lw=2, label=f\"ROC curve for {classes[i]} (area = {roc_auc[i]:0.2f})\")\n\nplt.plot([0, 1], [0, 1], 'k--', lw=2)\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc='lower right')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}